# -*- coding: utf-8 -*-
"""khelawan singh 0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CiJ79mqAz-THEkzMAOiM31aDMfBBB_5p
"""

#Upload all the  libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Give the dataset path
df = pd.read_csv('E:\data processing\housing.csv')

#print dataset
df.head()

"""# Subset of data to work with"""

housing = df[['LotFrontage','LotArea','LotShape','SalePrice','YrSold','SaleCondition','BsmtQual','GarageType','GarageArea','FireplaceQu']].copy()

housing.head(20)

#Finding the null values
housing.isnull().sum()

housing.isnull().sum()/len(housing)

# Finding mean and median for imputation

# finding missing values

housing.isnull().mean()

# Step 1 - Seprate the numerical data

housing1 = df[['LotFrontage','LotArea','SalePrice','YrSold','GarageArea']]
housing1.head(20)

# step 2 - calculate the mean for the seperated data set and display the coloum having maximum percentage of missing values.

housing1.isnull().mean()

# calculate the mean of colum lotFrontage.

mean = housing1.LotFrontage.mean()
mean

# calculate the median of the column LotFrontage.

median = housing1.LotFrontage.median()
median

# step 3 - Replaceing the missing values with mean and median on column LotFrontage.

housing1["LotFrontage_mean"] = housing1.LotFrontage.fillna(mean)
housing1["LotFrontage_median"] = housing1.LotFrontage.fillna(median)

housing1.head(20)

# plot graph for the data set to know the resultant data distribution

housing1.plot()

# ploting Graph for Lot Frontage column
housing1.LotFrontage.plot()

# PLOT THE LINE GRAPH FOR THE LotFrontage , LotFrontage_mean and LotFrontage_median

plt.rcParams["figure.figsize"] = [8,7]

fig=plt.figure()
ax = fig.add_subplot(111)

housing1.LotFrontage.plot(kind = 'kde' , ax=ax)
housing1.LotFrontage_mean.plot(kind='kde' , ax=ax , color ='red')
housing1.LotFrontage_median.plot(kind='kde' , ax=ax , color ='yellow')

lines,labels = ax.get_legend_handles_labels()
ax.legend(lines, labels , loc= 'best')

# end of imputation

# checking the dta is nornally distributed or not

housing1.LotFrontage.hist()

housing1.LotFrontage.hist(bins=60)

# data is normally distributed so compute end of distribution value

eod_value = housing1.LotFrontage.mean() + 3 + housing1.LotFrontage.std()
print(eod_value)

# create a new column age_eod and filling missing values with end of distribution value
housing1['LotFrontage_eod'] = housing1.LotFrontage.fillna(eod_value)
housing1.head(20)

# Printing first 20 rows after filling with eod_

# Plot KDE for Age and Age_eod

plt.rcParams["figure.figsize"] = [8,6]
fig = plt.figure()
ax = fig.add_subplot(111)
housing1['LotFrontage'] .plot(kind='kde', ax=ax)
housing1['LotFrontage_eod'] .plot(kind='kde', ax=ax)
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')

# plot histrogram to check the data distribution
housing.LotFrontage.hist()

# Filling with 99 and -1 on Age column

# create new columns age_99 and age_minus1 and filling missing values
housing1['LotFrontage_99'] = housing1.LotFrontage.fillna(99)
housing1['LotFrontage_minus1'] = housing1.LotFrontage.fillna(-1)
housing1.head(20)

# Ploting KDE

plt.rcParams["figure.figsize"] = [8,6]
fig = plt.figure()
ax = fig.add_subplot(111)
housing1['LotFrontage'] .plot(kind='kde', ax=ax)
housing1['LotFrontage_99'] .plot(kind='kde', ax=ax, color='red')
housing1['LotFrontage_minus1'] .plot(kind='kde', ax=ax, color='green')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')

# Again printing the subset of data to work

housing.head()

# seperating the categorical data
housing2 = df[['LotFrontage','LotShape','SaleCondition','BsmtQual','GarageType','FireplaceQu']]

housing2.head(20)

housing2.isnull().mean()

# drop coloum with more than 25% og missing data

housing2.drop('FireplaceQu',inplace=True,axis=1)
housing2.head(20)

# now we check the mode of BsmtQual
housing2.BsmtQual.mode()

housing2.BsmtQual.fillna('mode')

housing2.head(20)

# now we find the mode of LotFrontage

housing2.LotFrontage.mode()

housing2.LotFrontage.fillna('mode')

housing2.head(20)

housing2['LotFrontage_mode'] = housing2.LotFrontage.fillna(60)
housing2.head(20)

# letâ€™s plot the kernel density estimation plot for the
#original LotFrontage column and the LotFrontage column that contains the
# mode of the values in place of the missing values.
plt.rcParams["figure.figsize"] = [8,6]
fig = plt.figure()
ax = fig.add_subplot(111)
housing2['LotFrontage'] .plot(kind='kde', ax=ax)
housing2['LotFrontage_mode'] .plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')

# Missing Category Imputation
housing2.head(20)

housing2.BsmtQual.fillna('missing')

# plot bar graph
housing2.BsmtQual.value_counts().sort_values(ascending=False).plot.bar()
plt.xlabel('BsmtQual')
plt.ylabel('no of houses')

"""# ENCODING CATEGORICAL DATA

"""

import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams["figure.figsize"] = [8,6]
sns.set_style("darkgrid")
import pandas as pd
import numpy as np
df=pd.read_csv(r"E:\data processing\housing.csv")
df.head()

# extracting CATEGORICAL VARIABLES ( sampling)

housing = df[['HouseStyle','GarageType','SaleType','LotFrontage']]

housing.head(20)

# ENCODING CATEGORICAL DATA

# 1. One HOT Encoding
# 2. Frequency Encoding
# 3. Ordinal Encoding

# 1. ONE HOT ENCODING
# STEPS: 1. Find "Unique" values on each column
# 2. Find "Dummies" on each column


# Finding unique values on each categorical column
print(housing['HouseStyle'].unique())
print(housing['GarageType'].unique())
print(housing['SaleType'].unique())

# finding dummies on column "HouseStyle"

temp = pd.get_dummies(housing['HouseStyle'])
temp.head()

# priting actual and encoded values of column 'HouseStyle'
pd.concat([housing['HouseStyle'], pd.get_dummies(housing['HouseStyle'])], axis=1).head()

# finding dummies on column "GarageType"

temp = pd.get_dummies(housing['GarageType'])
temp.head()

# priting actual and encoded values of column 'GarageType'
pd.concat([housing['GarageType'], pd.get_dummies(housing['GarageType'])], axis=1).head()

# need N-1 columns in the one hot encoded dataset for a column that originally contained N unique labels.

temp = pd.get_dummies(housing['GarageType'], drop_first = True)
temp.head()

# Also, we can create one hot encoded column for null values in
# the actual column by passing True as a value for the dummy_na
# parameter.
temp = pd.get_dummies(housing['GarageType'], dummy_na = True ,drop_first = True)
temp.head()

# finding dummies on column "SaleType"

temp = pd.get_dummies(housing['SaleType'])
temp.head()

# priting actual and encoded values of column 'SaleType'

pd.concat([housing['GarageType'], pd.get_dummies(housing['SaleType'])], axis=1).head()

# Advantage
# No assumption about the dataset and all the categorical values can be successfully encoded.
#Disadvantage
# Feature space can become very large since a categorical column can have a lot of unique values.




# 2. FREQUENCY ENCODING
# Step1: Remove NULL/NA values on the categorical column.
# Step2: Call the value_counts() method on the categorical column, and then chain it with the to_dict() column to obtain the count for each unique label in th
# Step3 : Finally, call the map() method and pass it the dictionarycontaining the labels and count

housing.dropna(inplace = True)

value_counts = housing['GarageType'].value_counts(). to_dict()
print(value_counts)

# call the map()

housing['GarageType'] = housing['GarageType']. map(value_counts)
housing.head()

# we can also add percentage frequency by dividing the label count by the total number of rows as follows:

import pandas
import numpy
frequency_count = (housing['GarageType'].value_counts() / len('GarageType') ).to_dict()
print(frequency_count)


housing['GarageType'] = housing['GarageType']. map(frequency_count)
housing.head()

# ORDINAL ENCODING



housing= housing[["HouseStyle", "GarageType", "SaleType","LotFrontage"]]
housing.groupby(['GarageType'])['LotFrontage'].mean().sort_values()

ordered_cats = housing.groupby(['GarageType'])['LotFrontage'].mean().sort_values().index
cat_map= {k: i for i, k in enumerate(ordered_cats, 0)} # Dictionary creation:
housing['GarageType_ordered'] = housing['GarageType'].map(cat_map)
housing.head()

# Mean Encoding

# Step1: Identify the column which you want apply mean encoding
# Step2: Calculate the mean on categorical column.
# Step3: Use to_dict() method to convert the dataframe into dictionary
# Step4: Use map() method to transform and add the new mean column to the original dataset.


housing.groupby(['GarageType'])['LotFrontage'].mean()

mean_labels = housing.groupby(['GarageType'])['LotFrontage']. mean().to_dict()
housing['GarageType_mean'] = housing['GarageType'].map(mean_labels)
housing.head()

"""# Data Discretization

"""

# How to convert continuous numeric values into discrete intervals. # The process of converting continuous numeric values into discrete intervals is called discretization or binning.
# Examples: price, age, weight, etc. # Advantages:
# Helpful to handle Outliers. # With discretization, the outliers can be placed into tail intervals along with the remaining inlier values that occur at tails. # Discretization is particularly helpful in cases where you have skewed distribution of data.

# 1) Equal width Discretiation
# Step1: Select a column to apply discretization. # Step2: Plot a histogram for the column. Histogram shows the distribution of data (normal / skewed) # Step3: Find the total value of the column by subtracting the minimum value from the maximum value. # Step4: Divide the obtain value with no.of intervals/bins # Step5: The minimum value will be rounded off to floor, while the maximum value will be rounded off to ceil.
# The value will be rounded off to the nearest integer value.

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

df = pd.read_csv('E:\data processing\housing.csv')
df.head()

# selection subdata set to work with

housing = df[['LotFrontage','LotArea','LotShape','SalePrice','YrSold','SaleCondition','BsmtQual','GarageType','GarageArea','FireplaceQu','OverallQual']].copy()
housing.head(20)

# STEP1: Plot a histogram for the price column

sns.distplot(housing['SalePrice'])

# STEP2: find the total price range by subtracting the minimum price from the maximum price.
price_range = housing['SalePrice'].max() - housing['SalePrice'].min()
print(price_range )

# STEP3: find the length or width of each interval, we simply need to divide the price by the number of intervals.
price_range / 10

# STEP4: The minimum price will be rounded off to floor, while the maximum price will be rounded off to ceil.
# The price will be # rounded off to the nearest integer value.
lower_interval = int(np.floor( housing['SalePrice'].min()))
upper_interval = int(np.ceil( housing['SalePrice'].max()))
interval_length = int(np.round(price_range / 10))
print(lower_interval)
print(upper_interval)
print(interval_length)

# Data Discretization
# STEP 5: Next, Let's create the 10 bins for our dataset. To create bins,
# we will start with the minimum value, and add the bin interval or # Length to it. To get the second interval, the interval Length will # be added to the upper boundary of the first interval and so on. # The following script creates 10 equal width bins.

total_bins = [i for i in range(lower_interval, upper_interval+interval_length, interval_length)]
print(total_bins)

# STEP 6: Next, we will create string Labels for each bin. You can give any name to the bin Labels.

bin_labels = ['Bin_no_' + str(i) for i in range(1, len(total_bins))]
print(bin_labels)

# STEP 7: You can create the Pandas Libraries "cut()" method to convert
# the continuous column values to numeric bin values.
# You need  to pass the data column that you want to be discretized, along # with the bin intervals and the bin Labels.

housing['SalePrice_bins'] = pd.cut(x=housing['SalePrice'],bins=total_bins, labels=bin_labels, include_lowest=True)
housing.head(20)

# STEP 8: Next, let's plot a bar plot that shows the frequency of prices in each bin.

housing.groupby('SalePrice_bins')['SalePrice'].count().plot.bar()
plt.xticks(rotation=45)

# 2. EQUAL FREQUENCY DISCRETIZATION

# In equal frequency discretization, the bin width is adjusted automatically in such a way that
# each bin contains exactly the same number of records or has the same frequency.
# In equal frequency discretization, the bin interval may not be the same.
# Is a unsupervised discretization technique.
# Let's apply equal frequency discretization on the price column of the Diamonds dataset

# STEP1: Load the dataset. # STEP2: Select a column.
# STEP3: Use qout() method to convert a continuous column into equal frequency discretized bins.
# The qout() function returns quartiles, equal to the number of specified intervals along with the bins.
# You have to pass the dataset column, the number of intervals, the Labels as mandatory parameters for the "acut()" function.
# Step4: Create a dataframe that shows the actual values and quartile information.

# STEP5: print bins
# STEP6: Next, find the number of records per bin.
# Step7: Create a Pandas dataframe containing the bins

# STEP4: Print bins and quartile
discretised_price, bins = pd.qcut(housing['SalePrice'], 10, labels=None, retbins=True, precision=3, duplicates='raise')
pd.concat([discretised_price, housing['SalePrice']], axis=1).head(10)

# STEP5: Print bins

print(bins)
print(type (bins))

# STEP6: find the number of records per bin.
discretised_price.value_counts()

# Step7: Create a Pandas dataframe containing the bins # create 10 Labels

bin_labels = ['Bin_no_' +str(i) for i in range(1,11)]
print(bin_labels)

# print dataset
housing['SalePrice_bins'] = pd.cut(x=housing['SalePrice'],bins=bins, labels=bin_labels, include_lowest=True)
housing.head(20)

# Bar plot
housing.groupby('SalePrice_bins')['SalePrice'].count().plot.bar()
plt.xticks (rotation=45)

# 3. K-Means Discretization

# K-means discretization is another unsupervised discretization technique based on the K-means algorithm.
# A brief description of the K-Means algorithm is given below:
# 1. In the beginning, K random clusters of data points are created, where K is the number of bins or intervals.
# 2. Each data point is linked to the closest cluster center.
# 3. The centers of all the clusters are updated based on the associated data points

# STEP1 : Load dataset
# STEP2 : Select the column/attribute
# STEP3 : call the "KBinsDiscretizer()" method and pass it the number of bins.
# STEP4 : Use the "fit()" method on the obtained result and pass to the column/attribute

# Let's use K-Means discretization to discretize the price column of the Diamonds dataset.

from sklearn.preprocessing import KBinsDiscretizer

discretization = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')
discretization. fit(housing[['SalePrice']])

# Use "bin_edges" attribute to access the bins created via K-means clustering

intervals = discretization.bin_edges_.tolist()
print(intervals)

# Labeling bins

bin_labels = ['Bin_no_' +str(i) for i in range(1,11)]
print(bin_labels)

cut_bins =[ 34900.        , 117054.22907167, 160410.274379  , 207515.23665734,
       262815.94007217, 329335.04116968, 403199.20516304, 489155.78125   ,
       572192.25      , 678265.        , 755000.        ]

housing['SalePrice_bins'] = pd.cut(x=housing['SalePrice'], bins=cut_bins, labels=bin_labels, include_lowest=True)
housing.head(20)

housing.groupby('SalePrice_bins')['SalePrice'].count().plot.bar()
plt.xticks(rotation=45)

# 4. Decision Tree Discretization # Decision tree discretization is a type of supervised discretization algorithm.

# In decision tree discretization, bins are created based on the values in some other columns.
# In decision tree discretization, NO NEED to specify the number of bins or intervals.
# The decision tree identifies the optimal number of bins

# IMPORT DECISION TREE CLASSIFIER from SKLEARN
# To implement decision tree discretization, you can use the "DecisionTreeClassifier class from the "sklearn.tree" module.

from sklearn.tree import DecisionTreeClassifier

# Now call the "fit()" method on the class and pass the continuous column name and the column on the basis of
# which you want to create your bins.

tree_model = DecisionTreeClassifier(max_depth=3)
tree_model.fit(housing['SalePrice'].to_frame(), housing['OverallQual'])
housing['SalePrice_tree']= tree_model.predict_proba(housing['SalePrice'].to_frame())[:,1]
housing.head()

# Now find the unique probability values in the price_tree column

housing['SalePrice_tree'].unique()

# Plot the frequency of records per bin

housing.groupby(['SalePrice_tree'])['SalePrice'].count().plot.bar()
plt.xticks (rotation=45)

"""# Outlier Handling"""

# Housing dataset Loaded with Seaborn package importing
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

df = pd.read_csv("E:\data processing\housing.csv")
df.head()

housing=df[['LotFrontage','LotArea','LotShape','SalePrice','YrSold','SaleCondition','BsmtQual','GarageType','GarageArea','FireplaceQu']].copy()
housing.head(20)

housing[30:40]

# Plot box plot for LotFrontage after removing outliers

sns.boxplot(y="LotFrontage",data=housing)

# use the IQR method to find the upper and lower Limits
# to find the outliers in the LotFrontage column.

IQR = housing["LotFrontage"].quantile(0.75) - housing["LotFrontage"].quantile(0.25)
lower_LotFrontage_limit = housing["LotFrontage"].quantile(0.25) - (IQR * 1.5)
upper_LotFrontage_limit = housing["LotFrontage"].quantile(0.75) + (IQR * 1.5)
print(lower_LotFrontage_limit)
print(upper_LotFrontage_limit)

# Finding outlier values in SalePrice column

LotFrontage_outliers = np.where(housing["LotFrontage"] > upper_LotFrontage_limit, True, np.where(housing["LotFrontage"] < lower_LotFrontage_limit, True, False))

LotFrontage_outliers[1:40]

# 1. OUTERLIER TRIMMING (Removing the outlier values in LotFrontage column)

housing_without_LotFrontage_outliers = housing.loc[~(LotFrontage_outliers), ]
housing.shape, housing_without_LotFrontage_outliers.shape

housing_without_LotFrontage_outliers[30:40]

# Plot box plot for LotFrontage column after removing outliers

sns.boxplot( y='LotFrontage', data = housing_without_LotFrontage_outliers)



# 2. OUTLIER CAPPING
# The outliers are capped at certain minimum and maximum values.
# The rows containing the outliers are not removed from the dataset.
# We will again use the Inter Quartile Range technique to find the lower and upper Limit
# for the outliers in the fare column of the Housing dataset.

# Plot box plot for the SalePrice column in Housing dataset

sns.boxplot( y='SalePrice', data=housing)

# Now Let us see how the data is distributed in fare column
# Plot the histogram
sns.distplot(housing['SalePrice'])

# use the IQR method to find the upper and lower Limits to find the outliers in the SalePrice column.
IQR = housing["SalePrice"].quantile(0.75) - housing["SalePrice"].quantile(0.25)
lower_SalePrice_limit = housing["SalePrice"].quantile(0.25) - (IQR * 1.5)
upper_SalePrice_limit = housing["SalePrice"].quantile(0.75) + (IQR * 1.5)
print(lower_SalePrice_limit)
print(upper_SalePrice_limit)

# Replace the outlier values that are there in SalePrice column:
# The outliers (SalePrice values) greater than the upper limit with upper Limit
# The outliers (SalePrice values) less than the lower limit with Lower Limit

housing["SalePrice"]= np.where(housing["SalePrice"] > upper_SalePrice_limit, upper_SalePrice_limit,
np.where(housing["SalePrice"] < lower_SalePrice_limit, lower_SalePrice_limit, housing["SalePrice"]))

housing

housing[30:40]

# Plot the box plot to check any outliers in the SalePrice column

sns.boxplot( y='SalePrice', data=housing)

#3. Outlier Capping Using Mean and Std
#Instead of using the IQR method,
# The upper and lower thresholds for outliers can be calculated via the mean and standard deviation method.
# To find the upper threshold, the mean of the data is added to three times the standard deviation value.
# Similarly, to find the lower threshold, you have to multiply the standard deviation by 3, and then remove
# the result from the mean.

# Plot a box plot that displays the distribution of data in the LotFrontage column of the housing dataset.
sns.boxplot( y='LotFrontage', data=housing)

# Now find the upper and lower Thresholds for the LotFrontage column of the housing dataset using the mean and standard deviation capp
lower_LotFrontage_limit = housing["LotFrontage"].mean() - (3 * housing["LotFrontage"].std())
upper_LotFrontage_limit = housing[ "LotFrontage"].mean() + (3 * housing["LotFrontage"].std())

print(lower_LotFrontage_limit)
print(upper_LotFrontage_limit)

# Now replace the outlier values by the upper and lower limits.
housing["LotFrontage"]= np.where(housing["LotFrontage"] > upper_LotFrontage_limit, upper_LotFrontage_limit,
 np.where(housing["LotFrontage"] < lower_LotFrontage_limit, lower_LotFrontage_limit, housing["LotFrontage"]))

sns.boxplot( y='LotFrontage', data=housing)

# 4. OUTLIER CAPPING USING QUANTILES
# You can also use quantile information to set the lower and upper Limits to find outliers.
# For instance, we can set 0.05 as the lower limit and 0.95 as the upper Limit to find the outliers, which means that
# if the data point is within the first 5 percent Lower values or 5 percent highest values, we consider it as an outlier.

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# plot a box plot for the SalePrice column of the housing dataset.
sns.boxplot(y='SalePrice', data=housing)

#  setting 0.05 as the lower limit and 0.95 as the upper limit for the quantiles to find the outliers

lower_SalePrice_limit = housing["SalePrice"].quantile(0.05)
upper_SalePrice_limit = housing["SalePrice"].quantile(0.95)

print(lower_SalePrice_limit)
print(upper_SalePrice_limit)

# replace all the values greater than 50 in the LotFrontage column of the housing dataset by 50. Similarly, values
# Less than 10 have been arbitrarily replaced by 20.
housing["SalePrice"]= np.where(housing["SalePrice"] > 50, 50,
                     np.where(housing["SalePrice"] < 10, 10, housing["SalePrice"]))

# The output shows that anything beyond 112.07 is an outlier, and similarly, the fare value below 7.22 is also an outlier.
# Now replace the outlier values by the upper and lower limit.
housing["SalePrice"]= np.where(housing["SalePrice"] > upper_SalePrice_limit, upper_SalePrice_limit,
                     np.where(housing["SalePrice"] < lower_SalePrice_limit, lower_SalePrice_limit, housing["SalePrice"]))

housing.SalePrice[1:40]

# plot a box plot for the fare column of the housing dataset after removing outliers using the quantile method.
sns.boxplot( y='SalePrice', data=housing)

# 5. Outlier Capping using Custom Values
# SalePrice coloumn
print(housing.SalePrice.max())
print(housing.SalePrice.min())

# replace all the values greater than 50 in the LotFrontage column of the LotFrontage dataset by 50. Similarly, values
# Less than 10 have been arbitrarily replaced by 20.
housing["LotFrontage"]= np.where(housing["LotFrontage"] > 50, 50,
np.where(housing["LotFrontage"] < 10, 10, housing["LotFrontage"]))

# now print the maximum and minimum values for the LotFrontage column of the Titanic dataset.
print(housing.LotFrontage.max())
print(housing.LotFrontage.min())

# plot the box plot.
sns.boxplot(y='LotFrontage', data=housing)







"""# TRANSFORMATIONS"""

# STANDARDIZATION ( Z-Score Normalization)
 # Housing data set Loading
# Importing the housing  Dataset

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np


df=pd.read_csv("E:\data processing\housing.csv")
df.head()

# numerical colouum extracting to work with
hu=df[['YrSold','LotArea','OverallQual','SalePrice']]
hu.head(20)

# Statistical measures
hu.describe()   # You can see that the mean, min, and max values for the three columns are very different.

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(hu)
hu_scaled = scaler.transform(hu)

hu_scaled = pd.DataFrame (hu_scaled, columns = hu.columns)
hu_scaled.head()

sns.kdeplot(hu['OverallQual'])

sns.kdeplot(hu_scaled['OverallQual'])

# FARE COLUMN

sns.kdeplot(hu['SalePrice'])

sns.kdeplot(hu_scaled['SalePrice'])

sns.kdeplot(hu['YrSold'])

sns.kdeplot(hu_scaled['YrSold'])

sns.kdeplot(hu['LotArea'])

sns.kdeplot(hu_scaled['LotArea'])

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(hu)
hu_MMscaled = scaler.transform(hu)

hu_MMscaled = pd.DataFrame (hu_MMscaled, columns = hu.columns)
hu_MMscaled.head()

# KERNEL DENSITY PLOT

sns.kdeplot(hu_MMscaled['OverallQual'])

#IRIS DATA SET

from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler
import numpy as np
# use the iris dataset
X, y = load_iris(return_X_y=True)
print(X.shape)
# (150, 4) # 150 samples (rows) with 4 features/variables (columns)
# build the scaler model
scaler = MinMaxScaler()
# fit using the train set
scaler.fit(X)
# transform the test test
X_scaled = scaler.transform(X)
# Verify minimum value of all features
X_scaled.min(axis=0)
# array([0., O., 0., 0.])
# Verify maximum value of all features
X_scaled.max(axis=0)
# array([1., 1., 1., 1.])
# Manually normalise without using scikit-Learn
X_manual_scaled = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
# Verify manually VS scikit-Learn estimation
print(np.allclose(X_scaled, X_manual_scaled)) #True

import matplotlib.pyplot as plt
fig, axes = plt.subplots(1,2)
axes[0].scatter(X[:,0], X[:,1], c=y)
axes[0].set_title("Original data")
axes[1].scatter(X_scaled[:,0], X_scaled[:,1], c=y)
axes[1].set_title("MinMax scaled data")
plt.show()

